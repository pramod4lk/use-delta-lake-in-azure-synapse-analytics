{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Use delta tables for streaming data\r\n",
        "\r\n",
        " from notebookutils import mssparkutils\r\n",
        " from pyspark.sql.types import *\r\n",
        " from pyspark.sql.functions import *\r\n",
        "\r\n",
        " # Create a folder\r\n",
        " inputPath = '/data/'\r\n",
        " mssparkutils.fs.mkdirs(inputPath)\r\n",
        "\r\n",
        " # Create a stream that reads data from the folder, using a JSON schema\r\n",
        " jsonSchema = StructType([\r\n",
        " StructField(\"device\", StringType(), False),\r\n",
        " StructField(\"status\", StringType(), False)\r\n",
        " ])\r\n",
        " iotstream = spark.readStream.schema(jsonSchema).option(\"maxFilesPerTrigger\", 1).json(inputPath)\r\n",
        "\r\n",
        " # Write some event data to the folder\r\n",
        " device_data = '''{\"device\":\"Dev1\",\"status\":\"ok\"}\r\n",
        " {\"device\":\"Dev1\",\"status\":\"ok\"}\r\n",
        " {\"device\":\"Dev1\",\"status\":\"ok\"}\r\n",
        " {\"device\":\"Dev2\",\"status\":\"error\"}\r\n",
        " {\"device\":\"Dev1\",\"status\":\"ok\"}\r\n",
        " {\"device\":\"Dev1\",\"status\":\"error\"}\r\n",
        " {\"device\":\"Dev2\",\"status\":\"ok\"}\r\n",
        " {\"device\":\"Dev2\",\"status\":\"error\"}\r\n",
        " {\"device\":\"Dev1\",\"status\":\"ok\"}'''\r\n",
        " mssparkutils.fs.put(inputPath + \"data.txt\", device_data, True)\r\n",
        " print(\"Source stream created...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        " # Write the stream to a delta table\r\n",
        " \r\n",
        " delta_stream_table_path = '/delta/iotdevicedata'\r\n",
        " checkpointpath = '/delta/checkpoint'\r\n",
        " deltastream = iotstream.writeStream.format(\"delta\").option(\"checkpointLocation\", checkpointpath).start(delta_stream_table_path)\r\n",
        " print(\"Streaming to delta sink...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        " # Read the data in delta format into a dataframe\r\n",
        " df = spark.read.format(\"delta\").load(delta_stream_table_path)\r\n",
        " display(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        " # create a catalog table based on the streaming sink\r\n",
        " spark.sql(\"CREATE TABLE IotDeviceData USING DELTA LOCATION '{0}'\".format(delta_stream_table_path))"
      ]
    }
  ],
  "metadata": {
    "save_output": true,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "python"
    },
    "language_info": {
      "name": "python"
    },
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  }
}